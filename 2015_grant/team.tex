
Multi-agent systems involving tradeoffs between selfish and collective
outcomes have been studied extensively by computer
scientists~\cite{claus1998dynamics} and social scientists (Rand \&
Nowak, 2013)~\cite{rand2013human}.  However, in many natural
situations, multiple agents already share a goal (i.e., they are
already on the same team), and they must solve the difficult task of
delegating tasks to achieve that joint goal.  Classic game-theoretic
solution concepts such as Nash Equilibria cannot specify how this is
accomplished, yet humans manage to converge on solutions to
coordination problems without extensive training or learning
(Bacharach, 2006)~\cite{bacharach2006beyond}.  Motivated by a previous
work in psychology and philosophy investigating the question of joint
intentionality, we propose to design algorithms that can engage in
team reasoning.

\commentb{I feel like there is a smooth way to connect teams and other-regarding preferences, but I
don't yet understand enough of where the team idea is going to know how to tie it all in. Here are
some rough ideas, though.}

Teams are working with limited resources, and individuals, even when
partitioned tasks, may not all be able to accomplish their tasks
simultaneously or completely. They need to be able to consider how
their actions will affect their teammates actions as the final outcome
is determined by the team's collective action. Teams will also be
acting strategically against, and with, other entities and teams. This
involves modeling the other's objectives and decisions. Within a team,
if we assume that the players are different entities (applications,
robots, your toaster, a scheduling system, the thermostat in your
house,...), then we need to be realistic---their motivations and
incentives need to be aligned and considered.

