
\paragraph{Receding Horizon Control}

When computing an infinite horizon value function is too challenging, approximations can be used instead. One common form of approximation is {\em receding horizon control} (RHC). In RHC, the agent selects actions for each state according to a {\em finite horizon value function} for that state. A finite horizon value function defines the expected future discounted reward for only $h$ steps into the future (the horizon). We denote the finite horizon value functions and policy for horizon $h$ as $V^h(s)$, $Q^h(s,a)$, and $\pi^h(s)$. These values can be recursively built up by computing the value of $V^h$ with the value of $V^{h-1}$. Conventionally, in the base case, $V^0(s) = 0 \forall s$; that is, the value of states past the horizon provide zero value. However, the zero horizon value function may also be set to some heuristic value to give additional direction to an RHC agent. The name {\em receding horizon} refers to the fact that, after each decision, the planning horizon recedes one more step from where it was when the previous decision was made.


\paragraph{Receding Horizon Inverse Reinforcement Learning}

One limitation of standard IRL algorithms is that the outer optimization loop over reward function parameters has an inner loop in which planning for the newly selected reward function parameters must be performed. Since planning is often computationally challenging in complex domains, requiring it as an inner loop in the reward function optimization can result in IRL being a very computationally demanding problem, which can be especially problematic in settings where agents are learning and interacting simultaneously. {\em Receding horizon inverse reinforcement learning} (RHIRL)~\cite{macglashan15b} relaxes this constraint allowing the designer to choose the amount of planning computation time used by using a receding horizon controller for planning instead of a more standard infinite horizon planning algorithm. %

Using RHC with a short horizon in IRL has the interesting property of causing IRL to find a reward function that encodes information about the policy since the the RHC planning can only look ahead so far from any state. \jmnote{This description feels awkward, but I'm not sure how to convey the idea with out getting really wordy...}. For example, if the horizon in RHIRL is set to 1, RHIRL may find a reward function that is structurally similar to the infinite horizon value function for the actual reward function that motivated the input observed behavior.\footnote{Note that to find a reward function that is similar to the infinite horizon value function, the reward function family must have expressive enough parameters to fit it.} When RHIRL uses a short horizon, it may require more training data than infinite horizon IRL to accurately generalize to new states, but in various domains it was found that even a horizon of one can generalize better than supervised LfD algorithms that directly learn state-action preferences\cite{macglashanIntention15}. In our norm-learning setting, the properties of RHIRL will be especially useful for efficiently capturing behavior preferences that can be incorporated with the overall joint task goal.%

Note that RHIRL is a meta-IRL algorithm in that various different existing IRL approaches can be reformulated in the RHIRL setting by using a receding horizon controller. In the original work, it was demonstrated in the gradient ascent maximum likelihood setting~\cite{babes11}.


